# 			CS1507 lab2实验报告

### 																		PB20000027    高俊杰

### 一.  在 Tiny-ImageNet 数据集上训练 Resnet 模型  

#### 1.使用 torchsummary 库中的 summary 函数计算模型的中间结果大小 

~~~python
model = models.resnet18()
summary(model, 3, 64, 64)
~~~

结果: 

1. 卷积层和池化层：
   - 输入尺寸：3 * 64 * 64
   - 卷积层1输出尺寸：64 * 64 * 64
   - 池化层1输出尺寸：64 * 32 * 32
2. 残差块1：
   - 残差块1输入尺寸：64 * 32 * 32
   - 残差块1输出尺寸：64 * 32 * 32
3. 残差块2：
   - 残差块2输入尺寸：64 * 32 * 32
   - 残差块2输出尺寸：128 * 16 * 16
4. 残差块3：
   - 残差块3输入尺寸：128 * 16 * 16
   - 残差块3输出尺寸：256 * 8 * 8
5. 残差块4：
   - 残差块4输入尺寸：256 * 8 * 8
   - 残差块4输出尺寸：512 * 4 * 4
6. 平均池化层
   - 平均池化层输出尺寸：512 * 1 * 1

#### 2.代码改动

~~~
git diff > code_diff.txt
~~~

使用git diff工具生成代码改动说明

![1689786572194](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689786572194.png)

#### 3.  代码中增加 torch.utils.tensorboard 的代码  

 每个训练周期结束后，在验证集上评估模型的性能， 并使用writer.add_scalar()方法将训练集和验证集的损失和准确率写入TensorBoard的日志文件 ，以便后续分析和可视化。 

~~~python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(log_dir='/output/logs')//输出到bitahub的output文件夹

 # evaluate on validation set
        acc1, acc5, loss = validate(val_loader, model, criterion, args)
        writer.add_scalar('Loss/val', loss, epoch)
        writer.add_scalar('Acc@1/val', acc1, epoch)
        writer.add_scalar('Acc@5/val', acc5, epoch)

if i % args.print_freq == 0:
    progress.display(i + 1)
writer.add_scalar('Loss/train', losses.avg, epoch)
writer.add_scalar('Acc@1/train', top1.avg, epoch)
writer.add_scalar('Acc@5/train', top5.avg, epoch)
~~~

#### 4.分类训练并使用tensorboard绘制曲线

首先在bitahab中使用修改后的代码和数据集训练出三个模型,从上到下分别在cpu; 4_gpu; 1_gpu环境下得到

![1689789609162](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689789609162.png)

从output得到训练的模型文件

![1689789756790](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689789756790.png)

保存模型文件及logs日志到本地,在本地运行tensorboard查看loss和精度曲线

在此处遇到了unicode编码报错,原因是路径名不能包含中文.

![1689791501703](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689791501703.png)

曲线如下所示:目测没啥明显明显差异,测试集精度下降,损失率上升说明过拟合了

![1689791533184](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689791533184.png)![1689791553473](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689791553473.png)![1689791571844](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689791571844.png)颜色对应:

![1689791619028](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689791619028.png)

#### 5.  --evaluate对比两次评估的差异

代码：这段代码主要是用于输出预测与标签不同的图片到diff文件夹，若不存在diff则创建，只需遍历预测为0或1的样本即可满足要求。然后判断预测结果是否与目标标签是否一致，不一致则将预测错误的样本输出到diff文件夹，并修改文件名展示预测的错误结果。

~~~python
# compute output
                output = model(images)
                loss = criterion(output, target)
                if show_diff and i == 0:
                    output_0 = output.argmax(axis = 1)[target ==0].cpu().numpy()
                    target_0 = target[target == 0].cpu().numpy()
                    if not os.path.exists('diff'):
                        os.mkdir('diff')
                    for j in range(len(output_0)):
                        if output_0[j] != target_0[j]:
                            shutil.copyfile(f'./imagenet/val/n01443537/images/n01443537_{j}.JPEG', f'./diff/0_misclassified_to_{output_0[j]}.JPEG')
                    output_1 = output.argmax(axis = 1)[target ==1].cpu().numpy()
                    target_1 = target[target == 1].cpu().numpy()
                    if not os.path.exists('diff'):
                        os.mkdir('diff')
                    for j in range(len(output_1)):
                        if output_1[j] != target_1[j]:
                            shutil.copyfile(f'./imagenet/val/n01629819/images/n01629819_{j}.JPEG', f'./diff/1_misclassified_to_{output_1[j]}.JPEG')
~~~

实验结果：cpu：

![1689831818617](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689831818617.png)1_gpu：

![1689831852886](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1689831852886.png)

肉眼判断法找出预测不同的图片



### 二.  复现 Word-level Language Model 并讨论  

#### 1.训练和生成

训练：参考readme.md

~~~shell
python main.py --cuda --epochs 6 --model Transformer --lr 5
~~~

![fig1](D:\Users\Administrator\Documents\File\Archive\课程文件\大二下\python深度学习\大作业2\lab2\language\fig1.png)

生成：

~~~shell
python generate.py   
~~~

![fig2](D:\Users\Administrator\Documents\File\Archive\课程文件\大二下\python深度学习\大作业2\lab2\language\fig2.png)



#### 2.生成transfomer结构

安装 graphviz 软件包并使用以下代码生成结构图

~~~python
_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)
yhat = model(_input)
make_dot(yhat, params=dict(list(model.named_parameters()))).render("transformer_arch", format="pdf")
~~~



pdf版本收录于实验文件

![](D:\Users\Administrator\Desktop\lab2\t2\transformer_arch.jpg)

#### 3.  Transformer 和CNN 在捕捉上下文依赖上有什么差异？  

 Transformer和CNN在捕捉上下文依赖方面具有一些差异。Transformer通过自注意力机制建立全局依赖关系，适用于处理长距离依赖；而CNN通过卷积层的局部感受野和参数共享，在平移不变性的任务上表现出色。根据具体任务的需求和数据类型，选择适合的网络架构可以更好地捕捉上下文依赖关系。这些差异可以通过研究论文和深入学习相关领域的知识来进一步了解和探索。 